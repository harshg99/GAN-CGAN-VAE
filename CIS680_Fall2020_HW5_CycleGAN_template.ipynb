{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIS680_Fall2020_HW5_CycleGAN_template.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"E4v6rxBKYp7Z"},"source":["# Download the dataset and supporting codes and pre-trained models"]},{"cell_type":"markdown","metadata":{"id":"kIMdTpeGg-Vw"},"source":["https://drive.google.com/file/d/1D3m6I2S8xIvdOHBkvwSSFof5OTJZCDSt/view?usp=drivesdk\n","\n","https://drive.google.com/file/d/1CDRMyBA3Hk_gmbk1xkAm7aRHXBOQQigu/view?usp=drivesdk\n","\n","https://drive.google.com/file/d/1so6AQ0_Pg255COjAzEOOVPlDzTJFVQro/view?usp=drivesdk"]},{"cell_type":"code","metadata":{"id":"2CQQXc9JUUYU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637666912239,"user_tz":300,"elapsed":20212,"user":{"displayName":"Harsh Goel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10557291504800936508"}},"outputId":"0c94a19b-6d95-4468-f062-b5433ed1885d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"CW3wE6aNYekA","colab":{"base_uri":"https://localhost:8080/","height":396},"executionInfo":{"status":"error","timestamp":1637666929027,"user_tz":300,"elapsed":16792,"user":{"displayName":"Harsh Goel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10557291504800936508"}},"outputId":"6cc0b71f-df59-4b80-91d0-21c48e851a1d"},"source":["from google.colab import auth\n","auth.authenticate_user()\n","from googleapiclient.discovery import build\n","drive_service = build('drive', 'v3')\n","import io\n","import os\n","from googleapiclient.http import MediaIoBaseDownload\n","def download_file(fn, file_id):\n","    request = drive_service.files().get_media(fileId=file_id)\n","    downloaded = io.BytesIO()\n","    downloader = MediaIoBaseDownload(downloaded, request)\n","    done = False\n","    while done is False:\n","        # _ is a placeholder for a progress object that we ignore.\n","        # (Our file is small, so we skip reporting progress.)\n","        _, done = downloader.next_chunk()\n","    downloaded.seek(0)\n","    folder = fn.split('/')\n","    if len(folder) > 1:\n","        os.makedirs(folder[0], exist_ok=True)\n","    with open(fn, 'wb') as f:\n","        f.write(downloaded.read())\n","id_to_fn = {\n","# '1D3rH-gSxIECZnoyc5k77PEucKBVzoiBc': 'edges2shoes.zip',\n","'1D3m6I2S8xIvdOHBkvwSSFof5OTJZCDSt': 'edges2shoes.zip',\n","'1CDRMyBA3Hk_gmbk1xkAm7aRHXBOQQigu': 'CycleGAN_support.zip',\n","# '1so6AQ0_Pg255COjAzEOOVPlDzTJFVQro': 'test_case.zip',\n","'1l7gHikhCz64hYHL4n_Ps7nufQcddcRyR': 'test_case.zip'\n","\n","}\n","# download all files into the vm\n","for fid, fn in id_to_fn.items():\n","    print(\"Downloading %s from %s\" % (fn, fid))\n","    download_file(fn, fid)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading edges2shoes.zip from 1D3m6I2S8xIvdOHBkvwSSFof5OTJZCDSt\n"]},{"output_type":"error","ename":"HttpError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-307a578ea4e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_to_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading %s from %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-307a578ea4e8>\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(fn, file_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# _ is a placeholder for a progress object that we ignore.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# (Our file is small, so we skip reporting progress.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdownloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mnext_chunk\u001b[0;34m(self, num_retries)\u001b[0m\n\u001b[1;32m    755\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMediaDownloadProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHttpError\u001b[0m: <HttpError 404 when requesting https://www.googleapis.com/drive/v3/files/1D3m6I2S8xIvdOHBkvwSSFof5OTJZCDSt?alt=media returned \"File not found: 1D3m6I2S8xIvdOHBkvwSSFof5OTJZCDSt.\". Details: \"File not found: 1D3m6I2S8xIvdOHBkvwSSFof5OTJZCDSt.\">"]}]},{"cell_type":"code","metadata":{"id":"oIZPNwOJZUB7"},"source":["! unzip -q edges2shoes.zip\n","! unzip -q CycleGAN_support.zip\n","! unzip -q test_case.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rT6-1Ax8YyQr"},"source":["# CycleGAN training (TODO)"]},{"cell_type":"code","metadata":{"id":"3zQDWN_lMWJI"},"source":["import torch\n","import pdb\n","\n","D_A = torch.load('test_case/D_A.pt')\n","D_B = torch.load('test_case/D_B.pt')\n","G_BA = torch.load('test_case/G_BA.pt')\n","G_AB = torch.load('test_case/G_AB.pt')\n","valid = torch.load('test_case/valid.pt')\n","fake = torch.load('test_case/fake.pt')\n","fake_A = torch.load('test_case/fake_A.pt')\n","fake_B = torch.load('test_case/fake_B.pt')\n","real_A = torch.load('test_case/real_A.pt')\n","real_B = torch.load('test_case/real_B.pt')\n","criterion_GAN = torch.load('test_case/criterion_GAN.pt')\n","criterion_cycle = torch.load('test_case/criterion_cycle.pt')\n","\n","\n","\n","def loss_discriminator(fakefG, D, real2D, valid, fake, criterion_GAN):\n","    '''\n","    loss_discriminator function is applied to compute loss for discriminator D_A and D_B,\n","    For example, we want to compute loss for D_A. The loss is consisted of two parts: \n","    D(real_A) and D(G(real_B)). We want to penalize the distance between D(real_A) part and 1 and \n","    distance between D(G(real_B)) part and 0. \n","    We will want to first compute discriminator loss given real_A and valid, which is all 1.\n","    Then we want to forward real_A through G_AB network to get fake image batch \n","    and compute discriminator loss given fake batch and fake, which is all 0.\n","    Finall, add up these two loss as the total discriminator loss.\n","    '''\n","    clas_real = D(real)\n","    #Because we need to maximise\n","    loss_real =  criterion(clas_real.squeeze(),Valid_label)\n","    fake = G(noise)\n","    clas_fake = D(fake.detach())\n","    loss_fake = criterion(clas_fake.squeeze(),Fake_label)\n","    loss_D = loss_real + loss_fake\n","    return loss_D, fake\n","\n","    return loss_D\n","\n","\n","\n","def loss_generator(G, real2G, D, valid, criterion_GAN):\n","    '''\n","    loss_generator function is applied to compute loss for both generator G_AB and G_BA:\n","    For example, we want to compute the loss for G_AB.\n","    real2G will be the real image in domain A, then we map real2G into domain B to get fake B,\n","    then we compute the loss between D_B(fake_B) and valid, which is all 1.\n","    The fake_B image will also be one of the outputs, since we want to use it in the loss_cycle_consis.\n","    '''\n","    \n","    return loss_G, fake_o_G\n","\n","\n","\n","def loss_cycle_consis(G, fakefG, real, criterion_cycle):\n","    '''\n","    loss_cycle_consis function is applied to both cycle consistency loss:\n","    between recovered A and original A,\n","    between recovered B and original B.\n","    For example, we want to compute the cycle consistency loss between recovered A and original A.\n","    fake2G will be the generated image in domain B, then we map fake2G back into domain A to get recovered A,\n","    then we compute the loss between recovered A and original A\n","    '''\n","    \n","    return loss_cycle\n","\n","\n","# test case\n","\n","test_loss_GAN_AB = torch.load('test_case/loss_GAN_AB.pt')\n","test_loss_cycle_A = torch.load('test_case/loss_cycle_A.pt')\n","test_loss_D_A = torch.load('test_case/loss_D_A.pt')\n","loss_GAN_AB, fake_B = loss_generator(G_AB, real_A, D_B, valid, criterion_GAN)\n","loss_cycle_A = loss_cycle_consis(G_BA, fake_B, real_A, criterion_cycle)\n","loss_D_A = loss_discriminator(fake_A, D_A, real_A, valid, fake, criterion_GAN)\n","\n","\n","print('test case loss_D_A:', test_loss_D_A.item())\n","print('computed loss_D_A:', loss_D_A.item())\n","\n","print('test case test_loss_GAN_AB:', test_loss_GAN_AB.item())\n","print('computed test_loss_GAN_AB:', loss_GAN_AB.item())\n","\n","print('test case loss_cycle_A:', test_loss_cycle_A.item())\n","print('computed loss_cycle_A:', loss_cycle_A.item())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sM1-EnX11hXX"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from torch.autograd import Variable\n","from torch.utils import data\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch import nn, optim\n","from datasets import *\n","from models import *\n","import argparse, os\n","import itertools\n","import torch\n","import time\n","import pdb\n","import matplotlib.pyplot as plt\n","\n","\n","# Training Configurations\n","# (You may put your needed configuration here. Please feel free to add more or use argparse. )\n","train_img_dir = './edges2shoes/train/'\n","img_shape = (3, 128, 128)\n","n_residual_blocks = 6\n","num_epochs = 2\n","batch_size = 1\n","lr_rate = 0.0002      # Adam optimizer learning rate\n","betas = (0.5, 0.999)  # Adam optimizer beta 1, beta 2\n","lambda_cyc = 10.0 \t  # cycle loss weight\n","latent_dim = 8        # latent dimension for the encoded images from domain B\n","report_feq = 10        # Visualize image every 'report_feq' iters\n","visual_feq = 1000\n","save_feq = 1000      # Save models every 'save_feq' iters\n","\n","# Random seeds (optional)\n","torch.manual_seed(1); np.random.seed(1)\n","\n","# set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Normalize image tensor\n","def norm(image):\n","    return (image/255.0-0.5)*2.0\n","\n","# Denormalize image tensor\n","def denorm(tensor):\n","    return ((tensor+1.0)/2.0)*255.0\n","\n","train_dataset = Edge2Shoe(train_img_dir)\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size)\n","print('the length of training data:', len(train_dataset))\n","# Losses\n","criterion_GAN = torch.nn.MSELoss()\n","criterion_cycle = torch.nn.L1Loss()\n","\n","cuda = torch.cuda.is_available()\n","\n","input_shape = img_shape\n","\n","# Initialize generator and discriminator\n","G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n","G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n","D_A = Discriminator(input_shape)\n","D_B = Discriminator(input_shape)\n","\n","\n","if cuda:\n","    G_AB = G_AB.to(device)\n","    G_BA = G_BA.to(device)\n","    D_A = D_A.to(device)\n","    D_B = D_B.to(device)\n","    criterion_GAN.to(device)\n","    criterion_cycle.to(device)\n","\n","# Initialize weights\n","G_AB.apply(weights_init_normal)\n","G_BA.apply(weights_init_normal)\n","D_A.apply(weights_init_normal)\n","D_B.apply(weights_init_normal)\n","\n","\n","\n","# Define optimizer\n","optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr_rate, betas=betas)\n","optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr_rate, betas=betas)\n","optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr_rate, betas=betas)\n","\n","# For adversarial loss\n","Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n","\n","# loss recorder\n","running_loss_D_A = 0\n","running_loss_D_B = 0\n","running_loss_GAN_AB = 0\n","running_loss_GAN_BA = 0\n","running_loss_cycle = 0\n","running_total_loss = 0\n","\n","loss_D_A = []\n","loss_D_B = []\n","loss_GAN_AB = []\n","loss_GAN_BA = []\n","loss_cycle = []\n","total_loss = []\n","# Training\n","total_steps = len(train_loader)*num_epochs; step = 0\n","for e in range(num_epochs):\n","    start = time.time()\n","    for idx, data in enumerate(train_loader):\n","        ########## Process Inputs ##########\n","        edge_tensor, rgb_tensor = data\n","        edge_tensor, rgb_tensor = norm(edge_tensor).to(device), norm(rgb_tensor).to(device)\n","        real_A = edge_tensor; real_B = rgb_tensor;\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n","        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n","\n","        # ------------------\n","        #  Train Generators\n","        # ------------------\n","        G_AB.train(); G_BA.train()\n","        optimizer_G.zero_grad()\n","\n","        # Generator loss\n","        loss_GAN_AB, fake_B = loss_generator(G_AB, real_A, D_B, valid, criterion_GAN)\n","        loss_GAN_BA, fake_A = loss_generator(G_BA, real_B, D_A, valid, criterion_GAN)\n","        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n","\n","        # Cycle loss\n","        loss_cycle_A = loss_cycle_consis(G_BA, fake_B, real_A, criterion_cycle)\n","        loss_cycle_B = loss_cycle_consis(G_AB, fake_A, real_B, criterion_cycle)\n","        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n","\n","        # Total loss\n","        loss_G = loss_GAN + lambda_cyc * loss_cycle\n","\n","        loss_G.backward()\n","        optimizer_G.step()\n","\n","\n","        # -----------------------\n","        #  Train Discriminator A\n","        # -----------------------\n","\n","        optimizer_D_A.zero_grad()\n","\n","        # function to compute loss_D_A\n","        loss_D_A = loss_discriminator(fake_A, D_A, real_A, valid, fake, criterion_GAN)\n","        loss_D_A = loss_D_A / 2\n","        # update the D_A network\n","        loss_D_A.backward()\n","        optimizer_D_A.step()\n","\n","\n","        # -----------------------\n","        #  Train Discriminator B\n","        # -----------------------\n","        optimizer_D_B.zero_grad()\n","\n","        # function to compute loss_D_A\n","        loss_D_B = loss_discriminator(fake_B, D_B, real_B, valid, fake, criterion_GAN)\n","        loss_D_B = loss_D_B / 2\n","        # update the D_B network\n","        loss_D_B.backward()\n","        optimizer_D_B.step()\n","\n","\n","        \n","\n","\n","\n","        running_total_loss += (loss_GAN + lambda_cyc * loss_cycle + loss_D_A + loss_D_B).item()\n","\n","        running_loss_GAN_AB += loss_GAN_AB.item()\n","        running_loss_GAN_BA += loss_GAN_BA.item()\n","        running_loss_D_A += loss_D_A.item()\n","        running_loss_D_B += loss_D_B.item()\n","        running_loss_cycle += lambda_cyc * loss_cycle.item()\n","        ########## Visualization ##########\n","        if step % report_feq == report_feq-1:\n","            print('Train Epoch: {} {:.0f}% \\tTotal Loss: {:.6f} \\tLoss_G_AB: {:.6f}\\tLoss_G_BA: {:.6f}\\tLoss_cycle: {:.6f}\\tLoss_D_A: {:.6f}\\tLoss_D_B: {:.6f}'.format\n","                    (e+1, 100. * idx / len(train_loader), running_total_loss / report_feq, \n","                    running_loss_GAN_AB/report_feq, running_loss_GAN_BA/report_feq, \n","                    running_loss_cycle/report_feq, running_loss_D_A/report_feq, \n","                    running_loss_D_B/report_feq))\n","            running_loss_D_A = 0\n","            running_loss_D_B = 0\n","            running_loss_GAN_AB = 0\n","            running_loss_GAN_BA = 0\n","            running_loss_cycle = 0\n","            running_total_loss = 0\n","            end = time.time()\n","            print(e, step, 'T: ', end-start)\n","            start = end\n","        ########## Visualize Generated images ##########\n","        if step % visual_feq == 0:\n","            vis_fake_A = denorm(fake_A[0].detach()).cpu().data.numpy().astype(np.uint8)\n","            vis_fake_B = denorm(fake_B[0].detach()).cpu().data.numpy().astype(np.uint8)\n","            vis_real_B = denorm(real_B[0].detach()).cpu().data.numpy().astype(np.uint8)\n","            vis_real_A = denorm(real_A[0].detach()).cpu().data.numpy().astype(np.uint8)\n","            fig, axs = plt.subplots(2,2, figsize = (5,5))\t\n","            \n","            axs[0,0].imshow(vis_real_A.transpose(1,2,0))\n","            axs[0,0].set_title('real images')\n","            axs[0,1].imshow(vis_fake_B.transpose(1,2,0))\n","            axs[0,1].set_title('generated images')\n","            axs[1,0].imshow(vis_real_B.transpose(1,2,0))\n","            axs[1,1].imshow(vis_fake_A.transpose(1,2,0))\n","            plt.show()\n","        ########## Save Generators ##########\n","        if step % save_feq == save_feq-1:\n","            if not os.path.exists('models'): os.mkdir('models')\n","            torch.save(G_AB, '/content/models/G_AB.pt')\n","            torch.save(G_BA, '/content/models/G_BA.pt')\n","            # feel free to save checkpoint if you need retrain the model...\n","\n","        step += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yx1sPwiu_04L"},"source":["# CycleGAN testing (TODO)"]},{"cell_type":"code","metadata":{"id":"t_5OoGMfKFuU"},"source":["from torch.utils.data import DataLoader, TensorDataset\n","test_batch_size = 1\n","test_img_dir = './edges2shoes/val/'\n","test_dataset = Edge2Shoe(test_img_dir)\n","test_loader = DataLoader(test_dataset, batch_size=test_batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"naYmwWpq9Eg_"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","G_AB = torch.load('/content/models/G_AB.pt')\n","G_BA = torch.load('/content/models/G_BA.pt')\n","if cuda:\n","    G_AB = G_AB.to(device)\n","    G_BA = G_BA.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7m1b2H0pLOfu"},"source":["G_AB.eval()\n","G_BA.eval()\n","\n","################################\n","# Please visualize real_edge, fake_shoe, real_shoe, fake_edge in 2-by-2 grids:\n","\n","################################        \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vnLrUjjVfxA-"},"source":["# Quantitative Evaluation"]},{"cell_type":"markdown","metadata":{"id":"c-T0HP1fBO1j"},"source":["## FID Score computation"]},{"cell_type":"markdown","metadata":{"id":"Wgxl2Y9gsJ8_"},"source":["First, we have to create 6 datasets:\n","- Domain A real set 1\n","- Domain A real set 2\n","- Domain A generate set\n","\n","- Domain B real set 1\n","- Domain B real set 2\n","- Domain B generate set\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hBkdwX3sf2qb"},"source":["Create folder to save images and create dataset."]},{"cell_type":"code","metadata":{"id":"RKIC1KfjvUsH"},"source":["! mkdir real_A_1 real_A_2 gen_A\n","! mkdir real_B_1 real_B_2 gen_B"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYMyd8CuBPJz"},"source":["# First create test data loader\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision.utils import save_image\n","test_batch_size = 1\n","test_img_dir = './edges2shoes/val/'\n","test_dataset = Edge2Shoe(test_img_dir)\n","test_loader = DataLoader(test_dataset, batch_size=test_batch_size)\n","\n","# indicate the device we will use\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load learnt Generator G_AB and G_BA\n","G_AB = torch.load('/content/models/G_AB.pt').to(device)\n","G_BA = torch.load('/content/models/G_BA.pt').to(device)\n","\n","# the size of dataset, we want to evaluate on\n","evaluate_num = 100\n","\n","# make the gen data set and folder\n","real_A_set_1 = []\n","real_A_set_2 = []\n","real_B_set_1 = []\n","real_B_set_2 = []\n","\n","gen_set_A = []\n","gen_set_B = []\n","\n","for idx, data in enumerate(test_loader, 0):\n","    real_A, real_B = data\n","    # plt.imshow(real_A.type(torch.uint8).squeeze(0).cpu().permute(1,2,0))\n","    # plt.show()\n","    real_A, real_B = norm(real_A).to(device), norm(real_B).to(device)\n","    # plt.imshow(denorm(real_A).type(torch.uint8).squeeze(0).cpu().permute(1,2,0))\n","    # plt.imshow(denorm(real_A).squeeze(0).cpu().permute(1,2,0))\n","    # plt.show()\n","    # print(real_A.shape)\n","    \n","    if idx < evaluate_num:\n","        fake_A = G_BA(real_B)\n","        fake_B = G_AB(real_A)\n","        real_A_set_1.append(denorm(real_A.detach()))\n","        real_B_set_1.append(denorm(real_B.detach()))\n","        gen_set_A.append(denorm(fake_A.detach()))\n","        gen_set_B.append(denorm(fake_B.detach()))\n","        \n","        # plt.imshow(data[0].type(torch.uint8).squeeze(0).permute(1,2,0))\n","        # plt.imshow(denorm(real_A).squeeze(0).cpu().permute(1,2,0))\n","        # plt.imshow(denorm(real_A).type(torch.uint8).squeeze(0).cpu().permute(1,2,0))\n","        # plt.show()\n","        plt.imsave('./real_A_1/real_A' + str(idx) + '.png', denorm(real_A).type(torch.uint8).cpu().squeeze().permute(1,2,0).numpy())\n","        plt.imsave('./real_B_1/real_B' + str(idx) + '.png', denorm(real_B).type(torch.uint8).cpu().squeeze().permute(1,2,0).numpy())\n","        plt.imsave('./gen_A/gen_A' + str(idx) + '.png', denorm(fake_A).type(torch.uint8).cpu().squeeze().permute(1,2,0).numpy())\n","        plt.imsave('./gen_B/gen_B' + str(idx) + '.png', denorm(fake_B).type(torch.uint8).cpu().squeeze().permute(1,2,0).numpy())\n","        # save_image(denorm(real_A).type(torch.uint8).squeeze(), './real_A_1/real_A' + str(idx) + '.png', normalize=False)\n","\n","        # save_image(denorm(real_B).detach().squeeze(), './real_B_1/real_B' + str(idx) + '.png', normalize=False)\n","        # save_image(denorm(fake_A).detach().squeeze(), './gen_A/gen_A' + str(idx) + '.png', normalize=False)\n","        # save_image(denorm(fake_B).detach().squeeze(), './gen_B/gen_B' + str(idx) + '.png', normalize=False)\n","        # del real_A; del real_B; del fake_A; del fake_B\n","\n","    elif evaluate_num <= idx < 2*evaluate_num:\n","        real_A_set_2.append(real_A.detach())\n","        real_B_set_2.append(real_B.detach())\n","        plt.imsave('./real_A_2/real_A' + str(idx) + '.png', denorm(real_A).type(torch.uint8).cpu().squeeze().permute(1,2,0).numpy())\n","        plt.imsave('./real_B_2/real_B' + str(idx) + '.png', denorm(real_B).type(torch.uint8).cpu().squeeze().permute(1,2,0).numpy())\n","        # save_image(denorm(real_A).detach().squeeze(), './real_A_2/real_A' + str(idx) + '.png', normalize=False)\n","        # save_image(denorm(real_B).detach().squeeze(), './real_B_2/real_B' + str(idx) + '.png', normalize=False)\n","    \n","    if idx == 2*evaluate_num-1:\n","        break\n","# make 6 pytorch dataset\n","real_A_dataset_1 = TensorDataset(torch.cat(real_A_set_1))\n","real_A_dataset_2 = TensorDataset(torch.cat(real_A_set_2))\n","real_B_dataset_1 = TensorDataset(torch.cat(real_B_set_1))\n","real_B_dataset_2 = TensorDataset(torch.cat(real_B_set_2))\n","gen_dataset_A = TensorDataset(torch.cat(gen_set_A))\n","gen_dataset_B = TensorDataset(torch.cat(gen_set_B))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVQjy_tuGPdm"},"source":["## Compute FID score"]},{"cell_type":"code","metadata":{"id":"7hJpNaRcGOQl"},"source":["! pip install pytorch-fid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2q_mIuDK8wP"},"source":["print('computing FID score between real_edge_1 and real_edge_2')\n","! python -m pytorch_fid '/content/real_A_1' '/content/real_A_2' --gpu 0\n","\n","print('computing FID score between real_edge_1 and gen_edge')\n","! python -m pytorch_fid '/content/real_A_1' '/content/gen_A' --gpu 0\n","\n","print('computing FID score between real_shoe_1 and real_shoe_2')\n","! python -m pytorch_fid '/content/real_B_1' '/content/real_B_2' --gpu 0\n","\n","print('computing FID score between real_shoe_1 and gen_shoe')\n","! python -m pytorch_fid '/content/real_B_1' '/content/gen_B' --gpu 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNG7AkY_Gt61"},"source":["## Compute IS score"]},{"cell_type":"code","metadata":{"id":"qC7FTAYHFGV2"},"source":["from inception_score import inception_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print('IS score for real_edge_1 data set:')\n","print(inception_score(real_A_dataset_1, cuda=True, batch_size=64, resize=True, splits=1))\n","print('IS score for gen_edge data set:')\n","print(inception_score(gen_dataset_A, cuda=True, batch_size=64, resize=True, splits=1))\n","\n","\n","print('IS score for real_shoe_1:')\n","print(inception_score(real_B_dataset_1, cuda=True, batch_size=64, resize=True, splits=1))\n","print('IS score for gen_shoe data set:')\n","print(inception_score(gen_dataset_B, cuda=True, batch_size=64, resize=True, splits=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJcwTlhcDvmE"},"source":[""],"execution_count":null,"outputs":[]}]}